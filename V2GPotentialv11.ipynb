{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOVLs0QSi4kohN6nKOHWPGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikszn/V2G-Potential/blob/main/V2GPotentialv11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V2G SCENARIO ANALYSIS: ACADEMIC JUPYTER NOTEBOOK STRUCTURE"
      ],
      "metadata": {
        "id": "n7BjNkC34lW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Vehicle-to-Grid (V2G) Scenario Analysis Framework\n",
        "Complete Academic Implementation Starting from Raw EV Usage Data\n",
        "\n",
        "This notebook provides comprehensive analysis of V2G potential through:\n",
        "1. Raw data loading and validation\n",
        "2. Data preprocessing and V2G potential calculation\n",
        "3. Hourly aggregation and feature engineering\n",
        "4. Exploratory Data Analysis of usage patterns\n",
        "5. Market pricing analysis (wholesale vs imbalance markets)  \n",
        "6. Integrated scenario modeling\n",
        "7. Academic results presentation and discussion\n",
        "\n",
        "\n",
        "Author: Ikram Ahmed\n",
        "Institution: UCL\n",
        "Date: 13/09/25\n"
      ],
      "metadata": {
        "id": "oD4F7FFf4ibp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 0: IMPORT LIBRARIES\n"
      ],
      "metadata": {
        "id": "_6Q7YKgNRZhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from typing import Dict, Any, Tuple, List\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# Configure display options for academic presentation\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.precision', 3)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"📊 V2G Scenario Analysis Framework - Complete Academic Pipeline\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKjOLP_JCb56",
        "outputId": "bb94392a-cba8-4291-88a0-9dfebe1ddddb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 V2G Scenario Analysis Framework - Complete Academic Pipeline\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 1: RAW DATA LOADING AND INITIAL VALIDATION\n"
      ],
      "metadata": {
        "id": "qg5q09-zChs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_validate_raw_data(raw_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load and validate raw EV usage data with automatic column mapping for your format\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n1️⃣ RAW DATA LOADING AND VALIDATION\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Create working copy to avoid modifying original\n",
        "    df = raw_df.copy()\n",
        "\n",
        "    # Basic dataset structure\n",
        "    n_rows, n_cols = df.shape\n",
        "\n",
        "    print(f\"📊 Raw Dataset Dimensions: {n_rows:,} records × {n_cols} columns\")\n",
        "    print(f\"📋 Actual columns in your data:\")\n",
        "    for i, col in enumerate(df.columns):\n",
        "        print(f\"   {i+1}. '{col}'\")\n",
        "\n",
        "    # Column mapping for your specific data format\n",
        "    column_mapping = {\n",
        "        'ParticipantID': 'vehicle_id',\n",
        "        'BatteryChargeStartDate': 'start_time',\n",
        "        'BatteryChargeStopDate': 'end_time',\n",
        "        'Starting SoC (of 12)': 'start_soc_raw',\n",
        "        'Ending SoC (of 12)': 'end_soc_raw'\n",
        "    }\n",
        "\n",
        "    print(f\"\\n🔄 Column Mapping Attempt:\")\n",
        "\n",
        "    # Check exactly which columns exist and which don't\n",
        "    successful_mappings = {}\n",
        "    failed_mappings = {}\n",
        "\n",
        "    for old_name, new_name in column_mapping.items():\n",
        "        if old_name in df.columns:\n",
        "            df[new_name] = df[old_name]\n",
        "            successful_mappings[old_name] = new_name\n",
        "            print(f\"   ✅ '{old_name}' → '{new_name}'\")\n",
        "        else:\n",
        "            failed_mappings[old_name] = new_name\n",
        "            print(f\"   ❌ '{old_name}' NOT FOUND\")\n",
        "\n",
        "    # If any mappings failed, show what we have vs what we expected\n",
        "    if failed_mappings:\n",
        "        print(f\"\\n⚠️  COLUMN MAPPING ISSUES:\")\n",
        "        print(f\"   Failed to map: {list(failed_mappings.keys())}\")\n",
        "        print(f\"   Your actual columns: {list(df.columns)}\")\n",
        "\n",
        "        # Try fuzzy matching to suggest corrections\n",
        "        print(f\"\\n🔍 Possible matches:\")\n",
        "        for expected_col in failed_mappings.keys():\n",
        "            for actual_col in df.columns:\n",
        "                if expected_col.lower() in actual_col.lower() or actual_col.lower() in expected_col.lower():\n",
        "                    print(f\"   '{expected_col}' might be '{actual_col}'\")\n",
        "\n",
        "        raise ValueError(f\"Column mapping failed. Please check your column names match exactly.\")\n",
        "\n",
        "    # Verify all required columns now exist\n",
        "    required_columns = ['vehicle_id', 'start_time', 'end_time', 'start_soc_raw', 'end_soc_raw']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        print(f\"❌ Missing required columns after mapping: {missing_columns}\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "        raise ValueError(f\"Required columns missing: {missing_columns}\")\n",
        "\n",
        "    print(f\"✅ All required columns successfully mapped!\")\n",
        "\n",
        "    # Get vehicle count after mapping\n",
        "    n_vehicles = df['vehicle_id'].nunique()\n",
        "    print(f\"🚗 Vehicle Sample: {n_vehicles:,} unique participants\")\n",
        "\n",
        "        # Column mapping for your specific data format\n",
        "    column_mapping = {\n",
        "        'ParticipantID': 'vehicle_id',\n",
        "        'BatteryChargeStartDate': 'start_time',\n",
        "        'BatteryChargeStopDate': 'end_time',\n",
        "        'Starting SoC (of 12)': 'start_soc_raw',\n",
        "        'Ending SoC (of 12)': 'end_soc_raw'\n",
        "    }\n",
        "\n",
        "    print(f\"\\n🔄 Column Mapping and Standardization:\")\n",
        "\n",
        "    # Check if we have the expected columns for your data format\n",
        "    your_columns = list(column_mapping.keys())\n",
        "    missing_columns = [col for col in your_columns if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        print(f\"   ❌ Missing expected columns: {missing_columns}\")\n",
        "        print(f\"   Available columns: {list(df.columns)}\")\n",
        "        raise ValueError(f\"Expected columns from your data format not found: {missing_columns}\")\n",
        "\n",
        "    # Apply column mapping\n",
        "    for old_name, new_name in column_mapping.items():\n",
        "        if old_name in df.columns:\n",
        "            df[new_name] = df[old_name]\n",
        "            print(f\"   ✓ {old_name} → {new_name}\")\n",
        "\n",
        "    # Get vehicle count after mapping\n",
        "    n_vehicles = df['vehicle_id'].nunique()\n",
        "    print(f\"🚗 Vehicle Sample: {n_vehicles:,} unique participants\")\n",
        "\n",
        "    # Data type validation and conversion\n",
        "    print(f\"\\n🔍 Data Type Validation and Conversion:\")\n",
        "\n",
        "    # Convert timestamps\n",
        "    for time_col in ['start_time', 'end_time']:\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
        "            df[time_col] = pd.to_datetime(df[time_col])\n",
        "            print(f\"   ✓ Converted {time_col} to datetime\")\n",
        "\n",
        "    # Convert SoC from \"of 12\" to percentage\n",
        "    print(f\"\\n⚡ SoC Conversion (of 12 → percentage):\")\n",
        "\n",
        "    for soc_col in ['start_soc_raw', 'end_soc_raw']:\n",
        "        # Convert to numeric first\n",
        "        df[soc_col] = pd.to_numeric(df[soc_col], errors='coerce')\n",
        "        invalid_count = df[soc_col].isna().sum()\n",
        "        if invalid_count > 0:\n",
        "            print(f\"   ⚠️ {soc_col}: {invalid_count} invalid values found\")\n",
        "\n",
        "    # Convert to percentage (assuming max is 12)\n",
        "    df['start_soc'] = (df['start_soc_raw'] / 12.0) * 100\n",
        "    df['end_soc'] = (df['end_soc_raw'] / 12.0) * 100\n",
        "\n",
        "    print(f\"   ✓ Converted SoC values to percentage scale:\")\n",
        "    print(f\"     Start SoC range: {df['start_soc'].min():.1f}% - {df['start_soc'].max():.1f}%\")\n",
        "    print(f\"     End SoC range: {df['end_soc'].min():.1f}% - {df['end_soc'].max():.1f}%\")\n",
        "\n",
        "    # Validate SoC conversion makes sense\n",
        "    if df['start_soc'].max() > 100 or df['end_soc'].max() > 100:\n",
        "        print(f\"   ⚠️ Warning: Some SoC values exceed 100% after conversion\")\n",
        "\n",
        "    if df['start_soc'].min() < 0 or df['end_soc'].min() < 0:\n",
        "        print(f\"   ⚠️ Warning: Some SoC values are negative after conversion\")\n",
        "\n",
        "    # Temporal coverage analysis\n",
        "    date_min = df['start_time'].min()\n",
        "    date_max = df['end_time'].max()\n",
        "    total_days = (date_max - date_min).days + 1\n",
        "\n",
        "    print(f\"\\n📅 Temporal Coverage:\")\n",
        "    print(f\"   Period: {date_min.strftime('%Y-%m-%d')} to {date_max.strftime('%Y-%m-%d')}\")\n",
        "    print(f\"   Duration: {total_days} days\")\n",
        "\n",
        "    # Data quality assessment\n",
        "    print(f\"\\n📋 Data Quality Assessment:\")\n",
        "\n",
        "    # Check for duplicate records\n",
        "    duplicates = df.duplicated().sum()\n",
        "    print(f\"   Duplicate records: {duplicates:,} ({duplicates/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # Check for invalid time sequences\n",
        "    invalid_times = (df['end_time'] <= df['start_time']).sum()\n",
        "    print(f\"   Invalid time sequences: {invalid_times:,}\")\n",
        "\n",
        "    # Check for impossible SoC values (after conversion to %)\n",
        "    invalid_soc = ((df['start_soc'] < 0) | (df['start_soc'] > 100) |\n",
        "                   (df['end_soc'] < 0) | (df['end_soc'] > 100)).sum()\n",
        "    print(f\"   Invalid SoC values (after % conversion): {invalid_soc:,}\")\n",
        "\n",
        "    # Check for backwards SoC (discharge sessions)\n",
        "    discharge_sessions = (df['end_soc'] < df['start_soc']).sum()\n",
        "    charging_sessions = (df['end_soc'] > df['start_soc']).sum()\n",
        "\n",
        "    print(f\"   Charging sessions (SoC increase): {charging_sessions:,} ({charging_sessions/len(df)*100:.1f}%)\")\n",
        "    print(f\"   Discharge sessions (SoC decrease): {discharge_sessions:,} ({discharge_sessions/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # Missing data analysis\n",
        "    missing_data = df.isnull().sum()\n",
        "    total_missing = missing_data.sum()\n",
        "    print(f\"   Total missing values: {total_missing:,} ({total_missing/(len(df)*len(df.columns))*100:.1f}%)\")\n",
        "\n",
        "    if total_missing > 0:\n",
        "        print(f\"   Missing by column:\")\n",
        "        for col, missing_count in missing_data[missing_data > 0].items():\n",
        "            print(f\"     {col}: {missing_count:,} ({missing_count/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # Vehicle-level statistics\n",
        "    sessions_per_vehicle = df.groupby('vehicle_id').size()\n",
        "    print(f\"\\n🚗 Participant-Level Statistics:\")\n",
        "    print(f\"   Sessions per participant - Mean: {sessions_per_vehicle.mean():.1f}, Median: {sessions_per_vehicle.median():.1f}\")\n",
        "    print(f\"   Range: {sessions_per_vehicle.min()} - {sessions_per_vehicle.max()} sessions\")\n",
        "\n",
        "    # Session duration analysis\n",
        "    df['session_duration_hours'] = (df['end_time'] - df['start_time']).dt.total_seconds() / 3600\n",
        "    duration_stats = df['session_duration_hours'].describe()\n",
        "    print(f\"\\n⏱️ Session Duration Analysis:\")\n",
        "    print(f\"   Mean duration: {duration_stats['mean']:.1f} hours\")\n",
        "    print(f\"   Range: {duration_stats['min']:.1f} - {duration_stats['max']:.1f} hours\")\n",
        "\n",
        "    # SoC behavior analysis\n",
        "    df['soc_change'] = df['end_soc'] - df['start_soc']\n",
        "    soc_change_stats = df['soc_change'].describe()\n",
        "\n",
        "    print(f\"\\n🔋 SoC Change Analysis:\")\n",
        "    print(f\"   Mean SoC change: {soc_change_stats['mean']:.1f} percentage points\")\n",
        "    print(f\"   SoC change range: {soc_change_stats['min']:.1f} to {soc_change_stats['max']:.1f} pp\")\n",
        "\n",
        "    # Data cleaning recommendations\n",
        "    print(f\"\\n🧹 Data Cleaning Recommendations:\")\n",
        "\n",
        "    recommendations = []\n",
        "\n",
        "    if invalid_times > 0:\n",
        "        recommendations.append(f\"Remove {invalid_times} records with invalid time sequences\")\n",
        "\n",
        "    if invalid_soc > 0:\n",
        "        recommendations.append(f\"Review {invalid_soc} records with impossible SoC values\")\n",
        "\n",
        "    if discharge_sessions > charging_sessions * 0.1:  # More than 10% discharge sessions\n",
        "        recommendations.append(f\"Consider filtering {discharge_sessions} discharge sessions for V2G analysis\")\n",
        "\n",
        "    if duration_stats['max'] > 48:  # Sessions longer than 48 hours\n",
        "        long_sessions = (df['session_duration_hours'] > 48).sum()\n",
        "        recommendations.append(f\"Review {long_sessions} unusually long sessions (>48 hours)\")\n",
        "\n",
        "    for i, rec in enumerate(recommendations, 1):\n",
        "        print(f\"   {i}. {rec}\")\n",
        "\n",
        "    if not recommendations:\n",
        "        print(f\"   ✅ Data appears clean and ready for V2G analysis\")\n",
        "\n",
        "    return {\n",
        "        'n_records': len(df),\n",
        "        'n_vehicles': n_vehicles,\n",
        "        'n_days': total_days,\n",
        "        'date_range': (date_min, date_max),\n",
        "        'data_quality': {\n",
        "            'duplicates': duplicates,\n",
        "            'invalid_times': invalid_times,\n",
        "            'invalid_soc': invalid_soc,\n",
        "            'total_missing': total_missing,\n",
        "            'charging_sessions': charging_sessions,\n",
        "            'discharge_sessions': discharge_sessions\n",
        "        },\n",
        "        'sessions_per_vehicle': sessions_per_vehicle.describe().to_dict(),\n",
        "        'duration_stats': duration_stats.to_dict(),\n",
        "        'soc_stats': soc_change_stats.to_dict(),\n",
        "        'processed_dataframe': df  # Return the processed dataframe with standardized columns\n",
        "    }\n",
        ""
      ],
      "metadata": {
        "id": "KqK4OvExMTNN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 2: DATA PREPROCESSING AND V2G POTENTIAL CALCULATION\n"
      ],
      "metadata": {
        "id": "2ixcYdSPC8uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_v2g_potential(raw_df: pd.DataFrame, battery_capacity_kwh: float = 40.0) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate V2G export potential from raw EV usage data\n",
        "\n",
        "    This function transforms raw session data into V2G export calculations\n",
        "    by analyzing charging sessions and determining exportable energy\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    raw_df : pd.DataFrame\n",
        "        Raw EV usage data\n",
        "    battery_capacity_kwh : float\n",
        "        Assumed battery capacity in kWh (default: 40 kWh for older EVs)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame with V2G potential calculations added\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n2️⃣ V2G POTENTIAL CALCULATION\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Create working copy\n",
        "    df = raw_df.copy()\n",
        "\n",
        "    # Calculate energy metrics\n",
        "    print(f\"🔋 Calculating energy metrics with {battery_capacity_kwh} kWh battery assumption\")\n",
        "\n",
        "    # Energy charged during session (based on SoC change)\n",
        "    df['soc_change'] = df['end_soc'] - df['start_soc']\n",
        "    df['energy_charged_kwh'] = (df['soc_change'] / 100) * battery_capacity_kwh\n",
        "\n",
        "    # Only consider sessions where energy was added (charging sessions)\n",
        "    charging_sessions = df[df['energy_charged_kwh'] > 0].copy()\n",
        "\n",
        "    print(f\"   Total sessions: {len(df):,}\")\n",
        "    print(f\"   Charging sessions: {len(charging_sessions):,} ({len(charging_sessions)/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # Calculate V2G export potential with different reserve levels\n",
        "    reserve_levels = [20, 40, 60]  # Percentage of battery to keep in reserve\n",
        "\n",
        "    print(f\"\\n⚡ Calculating V2G export potential at different reserve levels:\")\n",
        "\n",
        "    for reserve_pct in reserve_levels:\n",
        "        # Available energy for export = energy above reserve level\n",
        "        charging_sessions[f'available_energy_reserve_{reserve_pct}'] = np.maximum(\n",
        "            0,\n",
        "            (charging_sessions['end_soc'] - reserve_pct) / 100 * battery_capacity_kwh\n",
        "        )\n",
        "\n",
        "        # Conservative V2G export assumption: export 80% of available energy\n",
        "        # (accounts for charging losses, user behavior, system constraints)\n",
        "        export_efficiency = 0.80\n",
        "        charging_sessions[f'v2g_export_potential_reserve_{reserve_pct}'] = (\n",
        "            charging_sessions[f'available_energy_reserve_{reserve_pct}'] * export_efficiency\n",
        "        )\n",
        "\n",
        "        # Statistics for this reserve level\n",
        "        total_potential = charging_sessions[f'v2g_export_potential_reserve_{reserve_pct}'].sum()\n",
        "        sessions_with_export = (charging_sessions[f'v2g_export_potential_reserve_{reserve_pct}'] > 0).sum()\n",
        "\n",
        "        print(f\"   {reserve_pct}% reserve: {total_potential:.1f} kWh potential, {sessions_with_export:,} sessions ({sessions_with_export/len(charging_sessions)*100:.1f}%)\")\n",
        "\n",
        "    # Add temporal features for hourly analysis\n",
        "    charging_sessions['start_hour'] = charging_sessions['start_time'].dt.hour\n",
        "    charging_sessions['start_dow'] = charging_sessions['start_time'].dt.dayofweek\n",
        "    charging_sessions['start_date'] = charging_sessions['start_time'].dt.date\n",
        "\n",
        "    # Calculate session-level participation flags\n",
        "    for reserve_pct in reserve_levels:\n",
        "        charging_sessions[f'can_participate_reserve_{reserve_pct}'] = (\n",
        "            charging_sessions[f'v2g_export_potential_reserve_{reserve_pct}'] > 0.1  # Minimum 0.1 kWh threshold\n",
        "        )\n",
        "\n",
        "    print(f\"\\n📊 V2G Participation Analysis:\")\n",
        "    for reserve_pct in reserve_levels:\n",
        "        participants = charging_sessions[f'can_participate_reserve_{reserve_pct}'].sum()\n",
        "        print(f\"   {reserve_pct}% reserve: {participants:,} sessions can participate ({participants/len(charging_sessions)*100:.1f}%)\")\n",
        "\n",
        "    return charging_sessions\n",
        "\n",
        "def create_hourly_aggregation(charging_sessions_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregate session-based data into hourly time series for V2G analysis\n",
        "\n",
        "    Creates hourly records showing V2G export potential across different\n",
        "    reserve levels and participation scenarios\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    charging_sessions_df : pd.DataFrame\n",
        "        Processed charging sessions with V2G calculations\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame with hourly V2G export aggregations\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n3️⃣ HOURLY AGGREGATION AND TIME SERIES CREATION\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Get temporal range\n",
        "    start_date = charging_sessions_df['start_time'].min().floor('H')\n",
        "    end_date = charging_sessions_df['start_time'].max().ceil('H')\n",
        "\n",
        "    print(f\"📅 Creating hourly time series from {start_date} to {end_date}\")\n",
        "\n",
        "    # Create complete hourly index\n",
        "    hourly_index = pd.date_range(start=start_date, end=end_date, freq='H')\n",
        "\n",
        "    # Get all vehicles\n",
        "    all_vehicles = charging_sessions_df['vehicle_id'].unique()\n",
        "\n",
        "    print(f\"🚗 Processing {len(all_vehicles):,} vehicles across {len(hourly_index):,} hours\")\n",
        "\n",
        "    # Create base hourly dataframe\n",
        "    hourly_records = []\n",
        "\n",
        "    for vehicle_id in all_vehicles:\n",
        "        vehicle_sessions = charging_sessions_df[charging_sessions_df['vehicle_id'] == vehicle_id]\n",
        "\n",
        "        for timestamp in hourly_index:\n",
        "            # Find sessions that overlap with this hour\n",
        "            hour_start = timestamp\n",
        "            hour_end = timestamp + timedelta(hours=1)\n",
        "\n",
        "            # Sessions that started in this hour (simplified approach)\n",
        "            hour_sessions = vehicle_sessions[\n",
        "                (vehicle_sessions['start_time'] >= hour_start) &\n",
        "                (vehicle_sessions['start_time'] < hour_end)\n",
        "            ]\n",
        "\n",
        "            # Aggregate V2G potential for this hour\n",
        "            record = {\n",
        "                'vehicle_id': vehicle_id,\n",
        "                'timestamp': timestamp,\n",
        "                'hour': timestamp.hour,\n",
        "                'day_of_week': timestamp.dayofweek,\n",
        "                'date': timestamp.date(),\n",
        "                'n_sessions': len(hour_sessions)\n",
        "            }\n",
        "\n",
        "            # Add export potential for each reserve level\n",
        "            reserve_levels = [20, 40, 60]\n",
        "            for reserve_pct in reserve_levels:\n",
        "                export_col = f'v2g_export_potential_reserve_{reserve_pct}'\n",
        "                if export_col in hour_sessions.columns:\n",
        "                    record[f'export_kwh_reserve_{reserve_pct}'] = hour_sessions[export_col].sum()\n",
        "                else:\n",
        "                    record[f'export_kwh_reserve_{reserve_pct}'] = 0.0\n",
        "\n",
        "            # Add participation flags\n",
        "            for reserve_pct in reserve_levels:\n",
        "                participate_col = f'can_participate_reserve_{reserve_pct}'\n",
        "                if participate_col in hour_sessions.columns:\n",
        "                    record[f'can_participate_reserve_{reserve_pct}'] = hour_sessions[participate_col].any()\n",
        "                else:\n",
        "                    record[f'can_participate_reserve_{reserve_pct}'] = False\n",
        "\n",
        "            hourly_records.append(record)\n",
        "\n",
        "    # Create hourly dataframe\n",
        "    hourly_df = pd.DataFrame(hourly_records)\n",
        "\n",
        "    # Data quality summary\n",
        "    total_export_40 = hourly_df['export_kwh_reserve_40'].sum()\n",
        "    active_hours = (hourly_df['export_kwh_reserve_40'] > 0).sum()\n",
        "    active_vehicles = hourly_df[hourly_df['export_kwh_reserve_40'] > 0]['vehicle_id'].nunique()\n",
        "\n",
        "    print(f\"\\n📊 Hourly Aggregation Summary:\")\n",
        "    print(f\"   Total records created: {len(hourly_df):,}\")\n",
        "    print(f\"   Active hours (40% reserve): {active_hours:,} ({active_hours/len(hourly_df)*100:.1f}%)\")\n",
        "    print(f\"   Total V2G potential (40% reserve): {total_export_40:.1f} kWh\")\n",
        "    print(f\"   Vehicles with V2G activity: {active_vehicles:,}/{len(all_vehicles):,}\")\n",
        "\n",
        "    # Temporal pattern summary\n",
        "    hourly_patterns = hourly_df.groupby('hour')['export_kwh_reserve_40'].sum()\n",
        "    peak_hour = hourly_patterns.idxmax()\n",
        "    peak_export = hourly_patterns.max()\n",
        "\n",
        "    print(f\"   Peak V2G hour: {peak_hour}:00 ({peak_export:.1f} kWh)\")\n",
        "\n",
        "    return hourly_df"
      ],
      "metadata": {
        "id": "ig6PQR25C9rC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 3: COMPREHENSIVE EXPLORATORY DATA ANALYSIS\n"
      ],
      "metadata": {
        "id": "VhmF84A2DEMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_comprehensive_eda(hourly_df: pd.DataFrame, raw_validation: Dict) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Comprehensive Exploratory Data Analysis of processed V2G data\n",
        "\n",
        "    Analyzes patterns in the processed hourly data to understand:\n",
        "    - Vehicle participation patterns\n",
        "    - Temporal usage trends\n",
        "    - Energy export distributions\n",
        "    - User behavior segmentation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n4️⃣ EXPLORATORY DATA ANALYSIS OF PROCESSED DATA\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    eda_results = {}\n",
        "    main_export_col = 'export_kwh_reserve_40'  # Use 40% reserve as primary metric\n",
        "\n",
        "    # --- 3.1 Vehicle Participation Analysis ---\n",
        "    print(\"\\n📋 4.1 Vehicle Participation Analysis\")\n",
        "\n",
        "    # Calculate vehicle-level aggregations\n",
        "    vehicle_totals = hourly_df.groupby('vehicle_id')[main_export_col].agg([\n",
        "        'count', 'sum', 'mean', 'std', 'max'\n",
        "    ]).round(2)\n",
        "\n",
        "    # Participation classification\n",
        "    vehicles_with_export = (vehicle_totals['sum'] > 0).sum()\n",
        "    participation_rate = vehicles_with_export / len(vehicle_totals)\n",
        "\n",
        "    print(f\"   Active V2G Participants: {vehicles_with_export}/{len(vehicle_totals)} vehicles ({participation_rate:.1%})\")\n",
        "\n",
        "    # Export distribution analysis\n",
        "    active_vehicles = vehicle_totals[vehicle_totals['sum'] > 0]\n",
        "\n",
        "    if len(active_vehicles) > 0:\n",
        "        export_stats = active_vehicles['sum'].describe()\n",
        "        print(f\"\\n   Export Distribution (Active Vehicles Only):\")\n",
        "        print(f\"     Mean export: {export_stats['mean']:.1f} kWh/vehicle (study period)\")\n",
        "        print(f\"     Median export: {export_stats['50%']:.1f} kWh/vehicle\")\n",
        "        print(f\"     Std deviation: {export_stats['std']:.1f} kWh\")\n",
        "        print(f\"     Range: {export_stats['min']:.1f} - {export_stats['max']:.1f} kWh\")\n",
        "\n",
        "        # Annualize the export values\n",
        "        study_days = raw_validation['n_days']\n",
        "        annual_factor = 365.25 / study_days\n",
        "        annual_export_stats = export_stats * annual_factor\n",
        "\n",
        "        print(f\"\\n   Annualized Export Estimates:\")\n",
        "        print(f\"     Mean annual export: {annual_export_stats['mean']:.0f} kWh/vehicle/year\")\n",
        "        print(f\"     Median annual export: {annual_export_stats['50%']:.0f} kWh/vehicle/year\")\n",
        "\n",
        "        # User segmentation\n",
        "        q75 = export_stats['75%']\n",
        "        q25 = export_stats['25%']\n",
        "        heavy_users = (active_vehicles['sum'] >= q75).sum()\n",
        "        light_users = (active_vehicles['sum'] <= q25).sum()\n",
        "\n",
        "        print(f\"\\n   User Segmentation:\")\n",
        "        print(f\"     Heavy users (≥75th percentile): {heavy_users} vehicles (≥{q75:.1f} kWh)\")\n",
        "        print(f\"     Light users (≤25th percentile): {light_users} vehicles (≤{q25:.1f} kWh)\")\n",
        "\n",
        "    eda_results['participation'] = {\n",
        "        'total_vehicles': len(vehicle_totals),\n",
        "        'active_vehicles': vehicles_with_export,\n",
        "        'participation_rate': participation_rate,\n",
        "        'export_distribution': export_stats.to_dict() if len(active_vehicles) > 0 else None,\n",
        "        'annual_export_per_vehicle': annual_export_stats['mean'] if len(active_vehicles) > 0 else 0,\n",
        "        'annual_export_per_active_vehicle': annual_export_stats['mean'] * len(vehicle_totals) / vehicles_with_export if vehicles_with_export > 0 else 0\n",
        "    }\n",
        "\n",
        "    # --- 3.2 Temporal Pattern Analysis ---\n",
        "    print(\"\\n📅 4.2 Temporal Pattern Analysis\")\n",
        "\n",
        "    # Hourly patterns\n",
        "    hourly_patterns = hourly_df.groupby('hour')[main_export_col].agg([\n",
        "        'sum', 'mean', 'count'\n",
        "    ]).round(2)\n",
        "\n",
        "    peak_hour = hourly_patterns['sum'].idxmax()\n",
        "    peak_export = hourly_patterns['sum'].max()\n",
        "    off_peak_hour = hourly_patterns['sum'].idxmin()\n",
        "\n",
        "    print(f\"   Hourly Patterns:\")\n",
        "    print(f\"     Peak export hour: {peak_hour}:00 ({peak_export:.1f} kWh total)\")\n",
        "    print(f\"     Lowest export hour: {off_peak_hour}:00 ({hourly_patterns['sum'].min():.1f} kWh total)\")\n",
        "\n",
        "    # Peak vs off-peak analysis\n",
        "    peak_hours = [17, 18, 19, 20]  # 5-8 PM\n",
        "    off_peak_hours = [1, 2, 3, 4, 5]  # 1-5 AM\n",
        "\n",
        "    peak_export_total = hourly_df[hourly_df['hour'].isin(peak_hours)][main_export_col].sum()\n",
        "    off_peak_export_total = hourly_df[hourly_df['hour'].isin(off_peak_hours)][main_export_col].sum()\n",
        "    peak_ratio = peak_export_total / max(off_peak_export_total, 1)\n",
        "\n",
        "    print(f\"     Peak period export (5-8 PM): {peak_export_total:.1f} kWh\")\n",
        "    print(f\"     Off-peak export (1-5 AM): {off_peak_export_total:.1f} kWh\")\n",
        "    print(f\"     Peak vs off-peak ratio: {peak_ratio:.1f}:1\")\n",
        "\n",
        "    # Weekly patterns\n",
        "    weekly_patterns = hourly_df.groupby('day_of_week')[main_export_col].sum()\n",
        "    weekday_avg = weekly_patterns[:5].mean()  # Mon-Fri\n",
        "    weekend_avg = weekly_patterns[5:].mean()  # Sat-Sun\n",
        "    weekend_ratio = weekend_avg / max(weekday_avg, 1)\n",
        "\n",
        "    print(f\"\\n   Weekly Patterns:\")\n",
        "    print(f\"     Weekday average: {weekday_avg:.1f} kWh/day\")\n",
        "    print(f\"     Weekend average: {weekend_avg:.1f} kWh/day\")\n",
        "    print(f\"     Weekend vs weekday ratio: {weekend_ratio:.2f}:1\")\n",
        "\n",
        "    eda_results['temporal'] = {\n",
        "        'peak_hour': peak_hour,\n",
        "        'peak_export': peak_export,\n",
        "        'peak_vs_offpeak_ratio': peak_ratio,\n",
        "        'weekend_vs_weekday_ratio': weekend_ratio,\n",
        "        'hourly_distribution': hourly_patterns['sum'].to_dict()\n",
        "    }\n",
        "\n",
        "    # --- 3.3 Reserve Level Comparison ---\n",
        "    print(\"\\n🔋 4.3 Reserve Level Impact Analysis\")\n",
        "\n",
        "    reserve_comparison = {}\n",
        "    for reserve_pct in [20, 40, 60]:\n",
        "        export_col = f'export_kwh_reserve_{reserve_pct}'\n",
        "        total_export = hourly_df[export_col].sum()\n",
        "        active_hours = (hourly_df[export_col] > 0).sum()\n",
        "        vehicles_participating = hourly_df[hourly_df[export_col] > 0]['vehicle_id'].nunique()\n",
        "\n",
        "        reserve_comparison[reserve_pct] = {\n",
        "            'total_export': total_export,\n",
        "            'active_hours': active_hours,\n",
        "            'vehicles_participating': vehicles_participating\n",
        "        }\n",
        "\n",
        "        print(f\"   {reserve_pct}% Reserve Level:\")\n",
        "        print(f\"     Total export: {total_export:.1f} kWh\")\n",
        "        print(f\"     Active hours: {active_hours:,}\")\n",
        "        print(f\"     Participating vehicles: {vehicles_participating}\")\n",
        "\n",
        "    # Calculate reserve level impacts\n",
        "    baseline_export = reserve_comparison[40]['total_export']  # 40% as baseline\n",
        "    for reserve_pct in [20, 60]:\n",
        "        impact = (reserve_comparison[reserve_pct]['total_export'] - baseline_export) / baseline_export * 100\n",
        "        print(f\"     {reserve_pct}% vs 40% reserve: {impact:+.1f}% export difference\")\n",
        "\n",
        "    eda_results['reserve_analysis'] = reserve_comparison\n",
        "\n",
        "    # --- 3.4 Energy Economics Baseline ---\n",
        "    print(\"\\n💰 4.4 Energy Economics Baseline\")\n",
        "\n",
        "    total_export = hourly_df[main_export_col].sum()\n",
        "    study_days = raw_validation['n_days']\n",
        "    annual_export_total = total_export * (365.25 / study_days)\n",
        "\n",
        "    annual_export_per_vehicle = annual_export_total / raw_validation['n_vehicles']\n",
        "    annual_export_per_active_vehicle = annual_export_total / max(vehicles_with_export, 1)\n",
        "\n",
        "    print(f\"   Energy Export Summary:\")\n",
        "    print(f\"     Total export (study period): {total_export:.1f} kWh\")\n",
        "    print(f\"     Annualized total export: {annual_export_total:.0f} kWh/year\")\n",
        "    print(f\"     Annual export per vehicle (all): {annual_export_per_vehicle:.1f} kWh/year\")\n",
        "    print(f\"     Annual export per active vehicle: {annual_export_per_active_vehicle:.1f} kWh/year\")\n",
        "\n",
        "    # Grid impact estimation\n",
        "    avg_hourly_export = total_export / (study_days * 24)\n",
        "    peak_hourly_export = hourly_df.groupby('timestamp')[main_export_col].sum().max()\n",
        "    capacity_factor = avg_hourly_export / max(peak_hourly_export, 1)\n",
        "\n",
        "    print(f\"\\n   Grid Impact Indicators:\")\n",
        "    print(f\"     Average hourly export: {avg_hourly_export:.2f} kWh/hour\")\n",
        "    print(f\"     Peak hourly export: {peak_hourly_export:.1f} kWh/hour\")\n",
        "    print(f\"     Export capacity factor: {capacity_factor:.2%}\")\n",
        "\n",
        "    eda_results['economics'] = {\n",
        "        'total_export_kwh': total_export,\n",
        "        'annual_export_per_vehicle': annual_export_per_vehicle,\n",
        "        'annual_export_per_active_vehicle': annual_export_per_active_vehicle,\n",
        "        'peak_hourly_export': peak_hourly_export,\n",
        "        'capacity_factor': capacity_factor\n",
        "    }\n",
        "\n",
        "    # --- 3.5 Data Processing Quality Assessment ---\n",
        "    print(\"\\n📊 4.5 Data Processing Quality Assessment\")\n",
        "\n",
        "    # Compare raw sessions to processed hours\n",
        "    raw_sessions = raw_validation['n_records']\n",
        "    processed_hours = len(hourly_df)\n",
        "    active_hours = (hourly_df[main_export_col] > 0).sum()\n",
        "\n",
        "    print(f\"   Data Transformation Summary:\")\n",
        "    print(f\"     Raw sessions processed: {raw_sessions:,}\")\n",
        "    print(f\"     Hourly records created: {processed_hours:,}\")\n",
        "    print(f\"     Hours with V2G activity: {active_hours:,} ({active_hours/processed_hours*100:.1f}%)\")\n",
        "\n",
        "    # Data retention analysis\n",
        "    vehicles_in_raw = raw_validation['n_vehicles']\n",
        "    vehicles_in_processed = hourly_df['vehicle_id'].nunique()\n",
        "    retention_rate = vehicles_in_processed / vehicles_in_raw\n",
        "\n",
        "    print(f\"     Vehicle retention: {vehicles_in_processed}/{vehicles_in_raw} ({retention_rate:.1%})\")\n",
        "\n",
        "    # --- 3.6 Key Insights Summary ---\n",
        "    print(\"\\n💡 4.6 Key Data Insights\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    insights = []\n",
        "\n",
        "    if participation_rate < 0.3:\n",
        "        insights.append(f\"Low participation rate ({participation_rate:.1%}) suggests V2G requires specific conditions\")\n",
        "    elif participation_rate > 0.7:\n",
        "        insights.append(f\"High participation rate ({participation_rate:.1%}) indicates strong V2G potential\")\n",
        "    else:\n",
        "        insights.append(f\"Moderate participation rate ({participation_rate:.1%}) shows selective V2G usage\")\n",
        "\n",
        "    if peak_ratio > 2:\n",
        "        insights.append(f\"Strong peak-period preference (ratio: {peak_ratio:.1f}:1) aligns with grid needs\")\n",
        "\n",
        "    if weekend_ratio < 0.8:\n",
        "        insights.append(f\"Lower weekend activity suggests work-related charging patterns\")\n",
        "\n",
        "    if annual_export_per_active_vehicle > 500:\n",
        "        insights.append(f\"High export per active vehicle ({annual_export_per_active_vehicle:.0f} kWh/year) demonstrates significant potential\")\n",
        "\n",
        "    if capacity_factor < 0.3:\n",
        "        insights.append(f\"Low capacity factor ({capacity_factor:.1%}) indicates concentrated usage patterns\")\n",
        "\n",
        "    for i, insight in enumerate(insights, 1):\n",
        "        print(f\"   {i}. {insight}\")\n",
        "\n",
        "    if not insights:\n",
        "        print(\"   Analysis reveals moderate V2G activity suitable for scenario modeling\")\n",
        "\n",
        "    return eda_results\n"
      ],
      "metadata": {
        "id": "8TyZeGY-DEkv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 4: PRICING DATA ANALYSIS AND MARKET CONTEXT  \n"
      ],
      "metadata": {
        "id": "bGyBuKkRDJli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "import time\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# Define SYSTEM_PARAMS if not already defined\n",
        "class SYSTEM_PARAMS:\n",
        "    PEAK_DEMAND_HOURS = [17, 18, 19, 20]\n",
        "\n",
        "class PricingManager:\n",
        "    \"\"\"Simple pricing data manager using current API with deflator adjustment\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_current_pricing_with_deflator(region_code: str = \"C\", gdp_deflator: float = 1.0) -> pd.DataFrame:\n",
        "        \"\"\"Load current API pricing and apply GDP deflator\"\"\"\n",
        "        print(f\"Loading current electricity pricing for region {region_code}...\")\n",
        "\n",
        "        try:\n",
        "            # Fetch current Agile pricing\n",
        "            url = f\"https://agilerates.uk/api/agile_rates_region_{region_code}.json\"\n",
        "            response = requests.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            data = response.json()\n",
        "            rates_df = pd.json_normalize(data[\"rates\"])\n",
        "\n",
        "            # Process pricing data\n",
        "            pricing_df = rates_df[[\n",
        "                \"deliveryStart\",\n",
        "                \"agileOutgoingRate.result.rate\"\n",
        "            ]].rename(columns={\n",
        "                \"deliveryStart\": \"timestamp_utc\",\n",
        "                \"agileOutgoingRate.result.rate\": \"export_price_p_per_kwh\"\n",
        "            })\n",
        "\n",
        "            pricing_df[\"timestamp_utc\"] = pd.to_datetime(pricing_df[\"timestamp_utc\"], utc=True)\n",
        "            pricing_df[\"timestamp_local\"] = pricing_df[\"timestamp_utc\"].dt.tz_convert(\"Europe/London\")\n",
        "            pricing_df[\"export_price_gbp_per_kwh\"] = pricing_df[\"export_price_p_per_kwh\"] / 100.0\n",
        "\n",
        "            # Create temporal features\n",
        "            pricing_df['day_of_week'] = pricing_df['timestamp_local'].dt.dayofweek\n",
        "            pricing_df['hour'] = pricing_df['timestamp_local'].dt.hour\n",
        "\n",
        "            # Create day-of-week × hour profile\n",
        "            pricing_profile = pricing_df.groupby(['day_of_week', 'hour'])['export_price_gbp_per_kwh'].mean().reset_index()\n",
        "\n",
        "            print(f\"✓ API pricing loaded: {len(pricing_df)} records aggregated to {len(pricing_profile)} hourly profiles\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"API failed: {e}\")\n",
        "            print(\"Using synthetic pricing patterns...\")\n",
        "\n",
        "            # Fallback synthetic pricing\n",
        "            pricing_data = []\n",
        "            for day_of_week in range(7):\n",
        "                weekend_factor = 0.85 if day_of_week >= 5 else 1.0\n",
        "\n",
        "                for hour in range(24):\n",
        "                    # Time-of-use pricing pattern\n",
        "                    if hour < 6:\n",
        "                        base_price = 0.08      # Night\n",
        "                    elif hour < 16:\n",
        "                        base_price = 0.12      # Day\n",
        "                    elif hour < 20:\n",
        "                        base_price = 0.25      # Peak\n",
        "                    else:\n",
        "                        base_price = 0.15      # Evening\n",
        "\n",
        "                    # Apply weekend discount and peak premiums\n",
        "                    price = base_price * weekend_factor\n",
        "                    if hour in [17, 18]:  # Peak premium\n",
        "                        price *= 1.2\n",
        "\n",
        "                    pricing_data.append({\n",
        "                        'day_of_week': day_of_week,\n",
        "                        'hour': hour,\n",
        "                        'export_price_gbp_per_kwh': price\n",
        "                    })\n",
        "\n",
        "            pricing_profile = pd.DataFrame(pricing_data)\n",
        "\n",
        "        # Apply GDP deflator\n",
        "        pricing_profile['export_price_gbp_per_kwh'] *= gdp_deflator\n",
        "\n",
        "        # Ensure complete 7×24 grid\n",
        "        idx = pd.MultiIndex.from_product([range(7), range(24)], names=['day_of_week', 'hour'])\n",
        "        pricing_profile = (pricing_profile.set_index(['day_of_week', 'hour'])\n",
        "                          .reindex(idx)\n",
        "                          .reset_index())\n",
        "        pricing_profile['export_price_gbp_per_kwh'] = pricing_profile['export_price_gbp_per_kwh'].interpolate().bfill().ffill()\n",
        "\n",
        "        print(f\"Applied GDP deflator: {gdp_deflator:.3f}\")\n",
        "        print(f\"Average export price: £{pricing_profile['export_price_gbp_per_kwh'].mean():.3f}/kWh\")\n",
        "\n",
        "        return pricing_profile\n",
        "\n",
        "class ImbalancePricingManager:\n",
        "    BASE_URL = \"https://api.bmreports.com/BMRS/B1770/v1\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _session():\n",
        "        s = requests.Session()\n",
        "        retries = Retry(total=5, backoff_factor=0.5,\n",
        "                       status_forcelist=[429,500,502,503,504],\n",
        "                       allowed_methods=[\"GET\"])\n",
        "        s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "        return s\n",
        "\n",
        "    @staticmethod\n",
        "    def _fetch_day_csv(api_key: str, settlement_date: str) -> pd.DataFrame:\n",
        "        params = {\"APIKey\": api_key, \"SettlementDate\": settlement_date, \"ServiceType\": \"csv\"}\n",
        "        r = ImbalancePricingManager._session().get(ImbalancePricingManager.BASE_URL,\n",
        "                                                   params=params, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        if not r.text or \"No data\" in r.text:\n",
        "            return pd.DataFrame()\n",
        "        df = pd.read_csv(io.StringIO(r.text))\n",
        "        df.columns = [c.strip().lower().replace(\" \",\"_\") for c in df.columns]\n",
        "        for col in (\"system_buy_price\",\"system_sell_price\",\"settlement_period\"):\n",
        "            if col in df.columns:\n",
        "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def fetch_imbalance_prices(api_key: str,\n",
        "                               start_date: str = \"2024-09-01\",\n",
        "                               end_date: str = \"2024-09-15\",\n",
        "                               price_choice: str = \"mid\") -> pd.DataFrame:\n",
        "        all_rows = []\n",
        "        for dt in pd.date_range(start_date, end_date, freq=\"D\"):\n",
        "            sd = dt.strftime(\"%Y-%m-%d\")\n",
        "            day = ImbalancePricingManager._fetch_day_csv(api_key, sd)\n",
        "            if day.empty:\n",
        "                time.sleep(0.25)\n",
        "                continue\n",
        "            day = day.dropna(subset=[\"settlement_period\"])\n",
        "            day[\"settlement_period\"] = day[\"settlement_period\"].astype(int)\n",
        "            day[\"hour\"] = ((day[\"settlement_period\"] - 1) // 2).clip(0,23)\n",
        "            day[\"day_of_week\"] = dt.dayofweek\n",
        "            sbp = day.get(\"system_buy_price\")\n",
        "            ssp = day.get(\"system_sell_price\")\n",
        "            if price_choice == \"sell\" and sbp is not None:\n",
        "                chosen = sbp\n",
        "            elif price_choice == \"buy\" and ssp is not None:\n",
        "                chosen = ssp\n",
        "            else:\n",
        "                chosen = (sbp + ssp) / 2.0 if sbp is not None and ssp is not None else (sbp if sbp is not None else ssp)\n",
        "            day[\"imbalance_price_gbp_per_kwh\"] = pd.to_numeric(chosen, errors=\"coerce\") / 1000.0\n",
        "            all_rows.append(day[[\"day_of_week\",\"hour\",\"imbalance_price_gbp_per_kwh\"]])\n",
        "            time.sleep(0.25)\n",
        "        if not all_rows:\n",
        "            raise RuntimeError(\"No BMRS imbalance data returned for the requested period.\")\n",
        "        raw = pd.concat(all_rows, ignore_index=True).dropna(subset=[\"imbalance_price_gbp_per_kwh\"])\n",
        "        hourly = (raw.groupby([\"day_of_week\",\"hour\"], as_index=False)\n",
        "                    .agg(imbalance_price_gbp_per_kwh=(\"imbalance_price_gbp_per_kwh\",\"mean\")))\n",
        "        # Ensure 7×24 grid but keep NaNs for missing bins\n",
        "        idx = pd.MultiIndex.from_product([range(7), range(24)], names=[\"day_of_week\",\"hour\"])\n",
        "        hourly = hourly.set_index([\"day_of_week\",\"hour\"]).reindex(idx).reset_index()\n",
        "        print(f\"✓ BMRS pulled. Rows: {len(hourly)}; NaNs: {hourly['imbalance_price_gbp_per_kwh'].isna().sum()}\")\n",
        "        return hourly\n",
        "\n",
        "print(\"✓ PricingManager and ImbalancePricingManager classes defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJJryAVrVesE",
        "outputId": "0245cc64-2c20-4785-fccf-78bc4f44481d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ PricingManager and ImbalancePricingManager classes defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_pricing_landscape() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load and analyze your actual electricity pricing data for V2G revenue modeling\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n5️⃣ ELECTRICITY PRICING ANALYSIS\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    pricing_results = {}\n",
        "\n",
        "    # --- Load your actual wholesale pricing data ---\n",
        "    print(\"\\n📈 5.1 Loading Your Wholesale Market Pricing\")\n",
        "\n",
        "    try:\n",
        "        wholesale_pricing = PricingManager.load_current_pricing_with_deflator(\n",
        "            region_code=\"C\",\n",
        "            gdp_deflator=1.0\n",
        "        )\n",
        "\n",
        "        wholesale_stats = wholesale_pricing['export_price_gbp_per_kwh'].describe()\n",
        "\n",
        "        print(f\"   ✅ Real wholesale pricing loaded!\")\n",
        "        print(f\"     Mean price: £{wholesale_stats['mean']:.3f}/kWh\")\n",
        "        print(f\"     Price range: £{wholesale_stats['min']:.3f} - £{wholesale_stats['max']:.3f}/kWh\")\n",
        "\n",
        "        # Peak vs off-peak analysis\n",
        "        peak_avg = wholesale_pricing[wholesale_pricing['hour'].isin([17,18,19,20])]['export_price_gbp_per_kwh'].mean()\n",
        "        off_peak_avg = wholesale_pricing[wholesale_pricing['hour'].isin([1,2,3,4,5])]['export_price_gbp_per_kwh'].mean()\n",
        "\n",
        "        pricing_results['wholesale'] = {\n",
        "            'source': 'Agile Rates API (Real Data)',\n",
        "            'mean_price': wholesale_stats['mean'],\n",
        "            'std_price': wholesale_stats['std'],\n",
        "            'min_price': wholesale_stats['min'],\n",
        "            'max_price': wholesale_stats['max'],\n",
        "            'peak_avg': peak_avg,\n",
        "            'off_peak_avg': off_peak_avg,\n",
        "            'volatility_ratio': peak_avg / off_peak_avg,\n",
        "            'data_points': len(wholesale_pricing)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️ Wholesale API failed: {e}\")\n",
        "        # Your fallback code here\n",
        "\n",
        "    # --- Load your actual imbalance pricing data ---\n",
        "    print(\"\\n⚡ 5.2 Loading Your Imbalance Market Pricing\")\n",
        "\n",
        "    try:\n",
        "        imbalance_pricing = ImbalancePricingManager.fetch_imbalance_prices(\n",
        "            api_key='zezhzizfgqavxqh',\n",
        "            start_date=\"2024-09-01\",\n",
        "            end_date=\"2024-09-15\"\n",
        "        )\n",
        "\n",
        "        imbalance_stats = imbalance_pricing['imbalance_price_gbp_per_kwh'].describe()\n",
        "\n",
        "        print(f\"   ✅ Real ELEXON imbalance pricing loaded!\")\n",
        "        print(f\"     Mean price: £{imbalance_stats['mean']:.3f}/kWh\")\n",
        "\n",
        "        pricing_results['imbalance'] = {\n",
        "            'source': 'ELEXON BMRS API (Real Data)',\n",
        "            'mean_price': imbalance_stats['mean'],\n",
        "            'std_price': imbalance_stats['std'],\n",
        "            'min_price': imbalance_stats['min'],\n",
        "            'max_price': imbalance_stats['max'],\n",
        "            'data_points': len(imbalance_pricing)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️ ELEXON API failed: {e}\")\n",
        "        # Fallback to synthetic based on wholesale\n",
        "\n",
        "    # Market analysis\n",
        "    market_premium = pricing_results['imbalance']['mean_price'] / pricing_results['wholesale']['mean_price']\n",
        "\n",
        "    pricing_results['market_analysis'] = {\n",
        "        'market_premium': market_premium\n",
        "    }\n",
        "\n",
        "    return pricing_results"
      ],
      "metadata": {
        "id": "2i7GdCYkDLgi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN EXECUTION FUNCTION\n"
      ],
      "metadata": {
        "id": "tNO3sR31DQwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_complete_raw_data_analysis(raw_df: pd.DataFrame, battery_capacity_kwh: float = 40.0) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Execute complete academic V2G analysis pipeline starting from raw data\n",
        "    \"\"\"\n",
        "    print(\"🎓 COMPLETE ACADEMIC V2G ANALYSIS - RAW DATA TO SCENARIOS\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Full pipeline from raw EV data to academic scenario analysis\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        # Section 1: Raw data validation\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        raw_validation = load_and_validate_raw_data(raw_df)\n",
        "\n",
        "        # Section 2: V2G potential calculation (FIXED - use processed dataframe)\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        processed_df = raw_validation['processed_dataframe']  # Get the processed dataframe\n",
        "        charging_sessions = calculate_v2g_potential(processed_df, battery_capacity_kwh)  # Pass the dataframe\n",
        "\n",
        "        # Section 3: Hourly aggregation\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        hourly_df = create_hourly_aggregation(charging_sessions)\n",
        "\n",
        "        # Section 4: Comprehensive EDA\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        eda_results = perform_comprehensive_eda(hourly_df, raw_validation)\n",
        "\n",
        "        # Section 5: Pricing analysis\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        pricing_results = analyze_pricing_landscape()\n",
        "\n",
        "        print(\"\\n✅ DATA PROCESSING AND EDA COMPLETE\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"Processed data ready for scenario analysis\")\n",
        "\n",
        "        # Compile results package\n",
        "        complete_results = {\n",
        "            'raw_data_validation': raw_validation,\n",
        "            'charging_sessions': charging_sessions,\n",
        "            'hourly_data': hourly_df,\n",
        "            'eda_results': eda_results,\n",
        "            'pricing_analysis': pricing_results,\n",
        "            'processing_metadata': {\n",
        "                'battery_capacity_kwh': battery_capacity_kwh,\n",
        "                'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
        "                'raw_records_processed': len(raw_df),\n",
        "                'hourly_records_created': len(hourly_df)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return complete_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ PROCESSING ERROR: {str(e)}\")\n",
        "        print(\"Please check your raw data format and required columns\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "G93yH2bRDSUm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN IT!"
      ],
      "metadata": {
        "id": "z-FqIcfvO1rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Load your data\n",
        "raw_df = pd.read_csv('/content/evchargedata.csv')\n",
        "print(f\"Loaded {len(raw_df):,} records\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn6gNYNwO3qJ",
        "outputId": "64197eb3-06b0-416a-cff7-9520d5fa0738"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 76,698 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the complete pipeline\n",
        "results = run_complete_raw_data_analysis(raw_df, battery_capacity_kwh=40.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XuRm6OuO_4Q",
        "outputId": "e1c89619-dcae-4490-9056-a047d442a067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎓 COMPLETE ACADEMIC V2G ANALYSIS - RAW DATA TO SCENARIOS\n",
            "================================================================================\n",
            "Full pipeline from raw EV data to academic scenario analysis\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "\n",
            "1️⃣ RAW DATA LOADING AND VALIDATION\n",
            "--------------------------------------------------\n",
            "📊 Raw Dataset Dimensions: 76,698 records × 5 columns\n",
            "📋 Actual columns in your data:\n",
            "   1. 'ParticipantID'\n",
            "   2. 'BatteryChargeStartDate'\n",
            "   3. 'BatteryChargeStopDate'\n",
            "   4. 'Starting SoC (of 12)'\n",
            "   5. 'Ending SoC (of 12)'\n",
            "\n",
            "🔄 Column Mapping Attempt:\n",
            "   ✅ 'ParticipantID' → 'vehicle_id'\n",
            "   ✅ 'BatteryChargeStartDate' → 'start_time'\n",
            "   ✅ 'BatteryChargeStopDate' → 'end_time'\n",
            "   ✅ 'Starting SoC (of 12)' → 'start_soc_raw'\n",
            "   ✅ 'Ending SoC (of 12)' → 'end_soc_raw'\n",
            "✅ All required columns successfully mapped!\n",
            "🚗 Vehicle Sample: 215 unique participants\n",
            "\n",
            "🔄 Column Mapping and Standardization:\n",
            "   ✓ ParticipantID → vehicle_id\n",
            "   ✓ BatteryChargeStartDate → start_time\n",
            "   ✓ BatteryChargeStopDate → end_time\n",
            "   ✓ Starting SoC (of 12) → start_soc_raw\n",
            "   ✓ Ending SoC (of 12) → end_soc_raw\n",
            "🚗 Vehicle Sample: 215 unique participants\n",
            "\n",
            "🔍 Data Type Validation and Conversion:\n",
            "   ✓ Converted start_time to datetime\n",
            "   ✓ Converted end_time to datetime\n",
            "\n",
            "⚡ SoC Conversion (of 12 → percentage):\n",
            "   ✓ Converted SoC values to percentage scale:\n",
            "     Start SoC range: 0.0% - 100.0%\n",
            "     End SoC range: 0.0% - 100.0%\n",
            "\n",
            "📅 Temporal Coverage:\n",
            "   Period: 2013-12-20 to 2015-11-29\n",
            "   Duration: 710 days\n",
            "\n",
            "📋 Data Quality Assessment:\n",
            "   Duplicate records: 69 (0.1%)\n",
            "   Invalid time sequences: 692\n",
            "   Invalid SoC values (after % conversion): 0\n",
            "   Charging sessions (SoC increase): 68,808 (89.7%)\n",
            "   Discharge sessions (SoC decrease): 11 (0.0%)\n",
            "   Total missing values: 0 (0.0%)\n",
            "\n",
            "🚗 Participant-Level Statistics:\n",
            "   Sessions per participant - Mean: 356.7, Median: 356.0\n",
            "   Range: 2 - 1010 sessions\n",
            "\n",
            "⏱️ Session Duration Analysis:\n",
            "   Mean duration: 3.0 hours\n",
            "   Range: -0.9 - 13.5 hours\n",
            "\n",
            "🔋 SoC Change Analysis:\n",
            "   Mean SoC change: 38.3 percentage points\n",
            "   SoC change range: -8.3 to 100.0 pp\n",
            "\n",
            "🧹 Data Cleaning Recommendations:\n",
            "   1. Remove 692 records with invalid time sequences\n",
            "\n",
            "================================================================================\n",
            "\n",
            "2️⃣ V2G POTENTIAL CALCULATION\n",
            "--------------------------------------------------\n",
            "🔋 Calculating energy metrics with 40.0 kWh battery assumption\n",
            "   Total sessions: 76,698\n",
            "   Charging sessions: 68,808 (89.7%)\n",
            "\n",
            "⚡ Calculating V2G export potential at different reserve levels:\n",
            "   20% reserve: 1575027.7 kWh potential, 68,624 sessions (99.7%)\n",
            "   40% reserve: 1138164.3 kWh potential, 67,893 sessions (98.7%)\n",
            "   60% reserve: 713775.5 kWh potential, 63,901 sessions (92.9%)\n",
            "\n",
            "📊 V2G Participation Analysis:\n",
            "   20% reserve: 68,624 sessions can participate (99.7%)\n",
            "   40% reserve: 67,893 sessions can participate (98.7%)\n",
            "   60% reserve: 63,901 sessions can participate (92.9%)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "3️⃣ HOURLY AGGREGATION AND TIME SERIES CREATION\n",
            "--------------------------------------------------\n",
            "📅 Creating hourly time series from 2013-12-20 10:00:00 to 2015-11-29 15:00:00\n",
            "🚗 Processing 215 vehicles across 17,022 hours\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the results\n",
        "print(f\"✅ Analysis complete!\")\n",
        "print(f\"Processed {results['processing_metadata']['raw_records_processed']:,} raw records\")\n",
        "print(f\"Created {results['processing_metadata']['hourly_records_created']:,} hourly records\")\n",
        "\n",
        "# Access specific components\n",
        "hourly_data = results['hourly_data']\n",
        "eda_insights = results['eda_results']"
      ],
      "metadata": {
        "id": "fc9p5IE9PIg8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}